/**
 * Gets the stage name for this job
 * @return String
 */
def getStageName() {
    return env.BRANCH_NAME == "master" ? "dev" : env.BRANCH_NAME
}

pipeline {

    agent { node { label 'ubuntu-18.04-worker' } }

    environment {
        STAGE = getStageName()
        SLACK_CHANNEL = '#data_platform'

        // Namespaces docker-compose to not compete with concurrent builds
        COMPOSE_PROJECT_NAME = "dewey-spark-ci-${env.BRANCH_NAME}-${currentBuild.id}"

        // The lowest-allowable pylint score
        PYLINT_THRESHOLD="4.80"

        // AWS Settings
        AWS_DEFAULT_REGION = 'us-east-1'
        AWS_ACCESS_KEY_ID = sh(script: "curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/jenkins_worker | jq -r '.AccessKeyId'", , returnStdout: true).trim()
        AWS_SECRET_ACCESS_KEY = sh(script: "curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/jenkins_worker | jq -r '.SecretAccessKey'", , returnStdout: true).trim()
        AWS_SESSION_TOKEN = sh(script: "curl -s http://169.254.169.254/latest/meta-data/iam/security-credentials/jenkins_worker | jq -r '.Token'", , returnStdout: true).trim()
    }

    stages {

        stage('Build') {
            steps {
                sh 'cd spark && make build-container'
            }
        }

        stage('QA') {
            parallel {  
                stage('Unit tests') {
                    steps {
                        sh 'cd spark && make test-python3'
                    }
                }

                stage('Lint') {
                    steps {
                        sh 'cd spark && make lint-python'
                    }
                }
            }
        }

        stage('Build package') {
            when { environment name: "STAGE", value: "dev" }
            steps {
                sh 'make package-spark'
            }
        }

        stage('Upload to S3') {
            when { environment name: "STAGE", value: "dev" }
            steps {
                sh 'aws s3 cp --sse AES256 dewey_spark.tar.gz s3://healthverityreleases/dewey/dewey_spark.tar.gz'
                sh 'aws s3 cp --sse AES256 spark/target/dewey.zip s3://healthverityreleases/dewey/dewey.zip'
            }
            post {
                success {
                    slackSend (color: '#00FF00',
                        message: "SUCCESSFUL\nJob: '${env.BUILD_TAG}'\nView build: (${env.BUILD_URL})",
                        channel: "${SLACK_CHANNEL}")
                }
                unsuccessful {
                    slackSend (color: '#FF0000',
                        message: "FAILED\nJob: '${env.BUILD_TAG}'\nView build: (${env.BUILD_URL})",
                        channel: "${SLACK_CHANNEL}")
                }
            }
        }
    }

    post {
        cleanup {
            cleanWs();
        }
    }
}
